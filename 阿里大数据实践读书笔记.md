#  阿里大数据实践

## 数据技术篇

### 1. 日志采集

#### 1.1 浏览器日志采集

##### 页面浏览日志

访客数uv和页面访问量pv的统计基础

**网页访问典型步骤:**

1. 用户点击链接;
2. 浏览器向服务器发起HTTP请求;
3. 服务器接收并解析请求;
4. 浏览器接收服务器的响应内容并展现给用户;

日志采集在第4步, 也就是浏览器开始解析文档时进行, 最直接思路: 在HTML文档增加日志采集节点, 当解析到此节点时触发一个特殊HTTP请求到日志采集服务器;

**日志采集过程**:

1. 客户端日志采集: 由一小段被植入页面HTML文档内的JS脚本执行, 植入动作有以下两种方式:
   - 业务服务器在响应业务请求时动态植入(阿里使用占比较高);
   - 在开发页面时由开发人员手动植入;
2. 客户端日志发送: 通常和日志采集放在同一个JS脚本内;
3. 服务端日志收集: 日志服务器收到日志请求后会立即发回请求成功的响应,避免对页面的正常加载造成影响; 日志请求内容会被写入一个日志缓冲区;
4. 服务端日志解析存档: 由一段专门的日志处理程序解析缓冲区的日志, 转存入标准的日志文件中并注入实时的消息通道内;

##### 页面交互日志

量化用户兴趣点或体验优化点, 阿里使用一套"黄金令箭"的采集方案来解决交互日志的采集问题;

"黄金令箭"是一个开放的基于HTTP协议的日志服务, 需要采集交互日志的业务经过如下步骤来实现采集:

1. 业务方在"黄金令箭"的元数据管理界面依次注册需要采集交互日志的业务,业务场景及场景下的交互采集点; 系统将生成对应的采集代码模板;
2. 业务方将采集代码植入目标页面, 并将采集代码与需要监测的交互行为绑定;
3. 当用户在页面上产生制定行为时, 采集代码和正常的业务互动代码一起被触发和执行;
4. 采集代码再采集动作完成后将日志发送到日志服务器,日志服务器原则上不做解析处理, 只做简单的转储;

##### 日志清洗和预处理

解析处理后的日志无法直接供下游使用,需要进行离线预处理的原因:

1. 识别流量攻击, 网络爬虫和流量作弊;
2. 数据缺项补正;
3. 无效数据剔除;
4. 日志隔离分发;

#### 1.2 无线客户端的日志采集

无线端的日志采集采用采集SDK来完成, 阿里内部多使用名为UserTrack的SDK来采集

移动端日志采集根据不同的用户行为分为不同的事件, 事件是移动端日子行为的最小单位, 常用包括页面事件和控件点击事件;

##### 页面事件

页面事件日志记录三类信息:

1. 设备及用户的基本信息
2. 被访问的页面信息
3. 访问的基本路径

对于页面事件, UT提供了两个接口, 分别在页面展现和页面退出时调用; 页面展现时会记录状态信息但是不发送日志, 当页面退出时, 退出接口会发送日志; 在这两个基础接口之外, 还提供了添加页面扩展信息的接口;

为了平衡采集,计算和分析的成本, UT提供了透传参数的功能, 即把当前页面的信息传递到下个页面甚至下个页面的日志中;

##### 控件点击及其他事件

逻辑相对页面事件简单, 记录基本的设备信息,用户信息,控件所在页面名称, 控件名称, 控件的业务参数等;

##### H5&Native日志统一

APP分为两种, 一种是纯Native APP, 一种是既有Native, 又有H5页面嵌入的APP, 即Hybird APP; 目前大多数APP均为Hyubird APP;

Native页面采用采集SDK进行日志采集, H5页面一般采用基于浏览器的页面日志采集方式进行采集, 从数据处理敏捷性,计算成本,合理性及准确性考虑, 需要对Native和H5日志进行统一处理;

阿里方案是将H5日志归到Native日志, 原因:

1. 采用采集SDK能采集到的更多的设备相关数据, 在移动端的数据分析中尤为重要;
2. SDK处理日志会先在本地缓存, 在网络上不佳时可延迟上报, 保证数据不丢失;

具体实现流程:

1. 利用JS脚本在H5页面实现日志采集;
2. 通过JS脚本将采集的数据打包成对象, 然后调用WebView框架的JSBridge接口,将埋点数据对象当做参数传入;
3. 采集SDK封装提供的接口, 将传入的内容转换成移动客户端日志格式

##### 设备标识

阿里巴巴接团无线设备唯一标识使用UTDID, 每台设备一个ID作为唯一标识;

### 2. 数据同步

**源业务数据类型**:

1. 关系型数据库的结构化数据,如MySQL, Oracle, DB2等
2. 非关系型数据库的非结构化数据, 如OceanBase,HBase, MongoDB等
3. 源于文件系统的结构化或非结构化数据,如阿里云对象存储OSS, 文件存储NAS等

**数据同步方式**: 直连同步, 数据文件同步, 数据库日志解析同步:

- **直连同步**: 通过定义好的接口和基于动态链接库的方式直接连接业务库, 配置简单, 但对源系统的性能影响较大; 当数据量大时, 性能较差, 不适合业务系统到数仓的同步;
- **数据文件同步**: 通过约定好的文件编码格式等,从源系统中生成数据的文本文件,由专门的文件服务器, 加载到目标数据库系统中; 文件服务器上传下载可能存在丢包或错误,因此会上传校验文件(数据量及文件大小等)供下游验证数据同步的准确性;
- **数据库日志解析同步**: 实现了实时与准实时同步的能力,对业务系统的性能影响也比较小, 广泛应用于从业务系统到数仓系统的增量数据同步; 存在问题:
  - 数据延迟
  - 投入较大
  - 数据漂移和遗漏

#### 2.1 阿里数仓的同步方式

阿里数仓的数据同步特点:

1. 数据来源的多样性;
2. 数据量大, 每天同步的数据量达到PB级别;

##### 离线批量数据同步

通过异构数据交换服务产品DataX, 将不同数据源的数据转换为中间状态(数据类型统一为字符串类型); DataX采用Framework+Plugin的开放式框架实现;

##### 实时数据同步

通过解析MySQL的binlog日志来实时获得增量的数据更新,并通过消息订阅模式来实现数据的实时同步; 阿里使用TimeTunnel(TT)系统作为实时数据传输平台, 具体来说TT是一种基于生产者,消费者和Topic消息标识的消息中间件, 将消息数据持久化到HBase的高可用,分布式数据交互系统;

#### 2.2 同步遇到的问题及解决方案

- **分库分表的处理**: 阿里的TDDL是分布式数据库的访问引擎, 通过建立中间状态的逻辑表来整合统一分库分表的访问
- **高效同步和批量同步**: 阿里的OneClick产品, 实现数据的一键化和批量化同步, 降低数据同步成本
- **增量与全量同步的合并**: 全外连接(full outer join)+数据全量覆盖重新加载(insert overwrite), 同时采用分区的模式
- **同步性能的处理**: 通过目标数据库的元数据估算同步任务的总线程数,通过系统预先定义的期望同步速度估算首轮同步的线程数,通过数据同步任务的业务优先级决定同步线程的优先级, 最终实现执行效率和稳定性;

##### 数据漂移

时间戳分为4类:

1. 数据库表中用来记录具体业务过程发生时间的时间戳(proc_time)
2. 数据库表中用来记录数据记录更新时间的时间戳(modified_time)
3. 数据库日志中用来标书数据记录更新时间的时间戳(log_time)
4. 标识数据记录被抽取到时间的时间戳(extract_time)

理论上4个时间应该是一致的, 但是基于以下原因会存在差异:

- 数据抽取需要时间, 所以extract_time晚于其他三个时间;
- 前台业务系统手工订正数据未更新modified_time
- 由于网络压力, log_time及modified_time晚于proc_time;

**处理方式**

1. 多获取后一天的数据
2. 通过多个时间戳字段限制时间来获取相对准确的数据
   - 根据log_time冗余前一天最后15分钟和后一天最初15分钟的数据, 并用modified_time过滤非当天数据
   - 根据log_time获取后一天15分钟数据,升序排列去重, 获取接近当天记录的变化数据
   - 将前两步结果全外链接,通过限制业务时间proc_time来获取需要的数据

### 3. 离线数据开发

#### 3.1 数据开发平台

数据开发工作流: 了解需求-模型设计-ETL开发-测试-发布上线-日常运维-任务下线

统一计算平台: MaxCompute, 是阿里自主研发的海量数据处理平台, 有四部分组成:

1. **客户端**: Web / SDK / CLT / IDE
2. **接入层**: 提供HTTP服务, Cache, 负载均衡
3. **逻辑层**: 即控制层, 核心部分, 包含以下三个角色:
   - Worker: 处理所有RESTful请求
   - Scheduler: 负责MaxCompute Instance的调度和拆解, 进行流控;
   - Executor: 负责MaxCompute Instance的执行, 向计算层提交计算任务
4. **计算存储层**: 
   - 计算层就是飞天内核, 包含Pangu(分布式文件系统), Fuxi(资源调度系统), Nuwa/ZK(Namespace服务), Shennong(监控模块)等
   - 存储层: 将元数据存储在另一个开放服务OTS中

**MaxCompute特点:**

1. 计算性能高且更加普惠;
2. 集群规模大且稳定性高
3. 功能组件非常强大
4. 安全性高

##### 统一开发平台:

<img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210409143405242.png" alt="image-20210409143405242" style="zoom:50%;width:50%" /><img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210409143443041.png" alt="image-20210409143443041" style="zoom:50%;width:50%" />

1. **在云端D2**: 集成任务开发, 调试及发布, 生产任务调度及大数据运维, 数据权限申请及管理等功能的一站式数据开发平台;
2. **SQLSCAN**: 代码校验, 有如下三类规则
   - 代码规范类规则,如表命名规范等
   - 代码质量类规则, 如调度参数使用检查等
   - 代码性能类规则, 如扫描大表提醒等
3. **DQC**: 数据质量中心, 有数据监控和数据清洗两大功能:
   - 数据监控规则分强弱, 潜规则会阻断任务的执行, 监控规则如:主键监控,表数据量及波动监控等
   - 数据清洗采用非侵入式的清洗策略, 在DQC配置清洗规则后在ODS层之后执行清洗, 洗掉的数据保存至DIRTY表归档
4. **在彼岸**: 自动化测试平台, 含数据对比, 数据分布, 数据脱敏组件;

#### 3.2 任务调度系统

两大核心模块: 

1. **调度引擎(Phoenix Engine)**: 根据任务节点属性及依赖关系进行实例化, 生成各类参数的实值, 并生成调度树;
2. **执行引擎(Alisa)**: 根据调度引擎生成的具体任务实例和配置信息, 分配CPU/内存/运行节点等资源, 在任务对应的执行环境中运行节点代码;

 **特点及应用:**

1. 调度配置
2. 定时调度
3. 周期调度
4. 手动运行
5. 补数据
6. 基线管理
7. 监控报警

### 4. 实时技术

**三类数据时效性:**

1. 离线, 延迟时间粒度为天
2. 准实时, 延迟时间粒度为小时;
3. 实时, 延迟时间粒度为秒;

离线和准实时都可以在批处理系统中实现, 只是调度周期不一样而已; 实时数据需要在流式处理系统中完成;

**流式数据处理特点:**

1. 时效性高
2. 常驻任务: 一旦启动就会一直运行, 直到人为终止, 因此计算成本比较高, 同时其数据源也是无界的;
3. 性能要求高: 如何保持高吞吐和低延时
4. 应用局限性: 无法替代离线处理

#### 4.1 流式技术架构

根据功能划分: 

1. **数据采集**:
   - 数据种类分两种: 数据库变更日志以及引擎访问日志
   - 基于数据大小限制/时间阈值限制, 按**批次**对数据进行采集
   - 采集的数据经过数据中间件分发给下游, 典型如Kafka, 阿里使用的是TimeTunnel;
2. **数据处理**
   - 阿里使用的阿里云提供的StreamCompute, 业界开源的有Storm,S4, Spark Streaming, Flink
3. **数据存储**
   - 存储数据有三类: 中间计算结果, 最终结果数据, 维表数据
   - 一般使用HBase, Tair, MongoDB等列式存储系统, 以满足多并发读写以及低延时要求
4. **数据服务**
   - 阿里使用同一的数据服务OneService;

#### 4.2 流式数据模型

与离线建模非常类似, 实时建模也分为5层: ODS, DWD, DWS, ADS,DIM:

1. ODS: 直接从业务系统采集的最原始数据, 与离线的源头统一;
2. DWD: 根据业务过程建模出来的实时事实明细层, 对于访问日志类数据,可回流到离线系统供下游使用;
3. DWS: 根据各垂直业务通用的维度进行指标汇总, 存放在实时通用汇总层;
4. ADS: 个性化维度汇总层, 对于不是特别通用的统计维度数据放在此层中;
5. DIM: 实时维表层, 通常数据是从离线维表层导出来的;

### 5. 数据服务

#### 5.1 服务架构演进

2010 DWSOA --> 2012 OpenAPI --> 2013 SmartDQ --> OneService

1. **DWSOA**: 将业务方对数据的需求通过SOA服务的方式暴露出去
   - 优点: 实现起来比较简单
   - 缺点: 接口粒度比较粗, 灵活性不高,扩展性查,复用率低; 开发效率不高, 无法快速响应业务;
2. **OpenAPI**: 将数据按照其统计粒度进行聚合, 同样维度的数据形成一张逻辑表,采用同样的接口描述
   - 优点: 有效收敛了接口数量
   - 缺点: 维度数量不可控, 随着时间推移越来越多, 带来了大量对象关系映射的维护工作量
3. **SmartDQ**: 在OpenAPI基础上再抽象一层, 用DSL来描述取数需求(采;用标准SQL语法)
   - 优点: 大大降低了数据服务的维护成本, 只需检查SQL的工作量
   - 缺点: 只能满足简单的查询服务需求, 不能解决复杂的业务逻辑;
4. **OneService**: 提供多种服务类型来满足用户的需求, 分别为OneService-SmartDQ, OneService-Lego, OneService-iPush, OneService-uTiming
   - Lego采用插件化方式开发服务, 可满足个性化的取数业务场景
   - iPush主要提供WebSocket和long polling两种方式, 其应用场景主要是商家端实时直播;
   - uTiming主要提供即时任务和定时任务两种模式, 满足用户运行大数据量任务的需求;

### 6. 数据挖掘

#### 6.1 数据挖掘算法平台

该平台架构与阿里云MaxCompute, GPU等计算集群至上, 汇集了阿里大量优质的分布式算法, 包括数据处理, 特征工程, 机器学习算法,文本算法等;

阿里算法平台选用MPI作为基础计算框架, 集成了绝大部分业界主流的机器学习算法:

<img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210409152415925.png" alt="image-20210409152415925" style="zoom: 67%;" />

#### 6.2 数据挖掘中台体系

##### 挖掘数据中台:

数据挖掘过程中包含两类数据: 特征数据和结果数据

挖掘数据中台划分为三层: 特征层(FDM),中间层和应用层(ADM), 其中中间层包括个体中间层(IDM)和关系中间层(RDM)

<img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210409152741722.png" alt="image-20210409152741722" style="zoom:67%;" />

1. **FDM层**: 用于存储在模型训练前常用的特征指标, 并进行统一的清洗和去噪处理
2. **IDM层**: 个体挖掘指标中间层, 面向个体挖掘场景, 用于存储通用性强的结果数据
3. **RDM层**: 关系挖掘指标中间层, 面向关系挖掘场景, 用于存储通用性强的结果数据
4. **ADM层**: 用来沉淀比较个性偏应用的数据挖掘指标

通过数据挖掘中台的建设,能大幅度节省特征工程的工作时间, 并减少重复的基础数据挖掘工作;

##### 挖掘算法中台

从各种各样的挖掘场景中抽象出有代表性的几类场景, 并形成相应的方法论和实操模板

<img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210409153348537.png" alt="image-20210409153348537" style="zoom:67%;" />

## 数据模型篇

### 7. 大数据领域建模综述

#### 7.1 为什么需要数据建模

数据模型就是**数据组织和存储**的方法, 它强调从业务,数据存取和使用角度合理存储数据; 好的数据模型可以带来性能/成本/效率/质量的优化;

#### 7.2 数仓与关系型数据库

数据仓库系统依托强大的关系数据库能力存储和处理数据, 其采用的数据模型方法也是基于关系数据库理论的;

#### 7.3 OLTP和OLAP系统的区别

- OLTP系统通常面向的主要数据操作是随机读写,主要采用满足3NF的实体关系模型存储数据, 从而在数据处理中解决数据的冗余和一致性问题
- OLAP系统面向的主要数据操作是批量读写, 事务处理中的一致性不是OLAP所关注的, 其主要关注数据的整合, 以及一次性的复杂大数据查询和处理中的性能;

#### 7.4 典型的数仓建模方法论

##### ER模型

从全企业的高度设计的一个3NF模型, 用实体关系(ER)模型描述企业业务, 具有以下特点:

- 需要全面了解企业业务和数据
- 实施周期非常长
- 对建模人员的能力要求非常高

建模步骤分为三个阶段:

1. 高层模型: 高度抽象的模型, 描述主要的主题及主题间的关系, 用于描述业务总体概况
2. 中层模型: 细化主题的数据项
3. 物理模型(底层模型): 在中间层基础上, 考虑物理存储,同时基于性能和平台特点进行物理属性的设计

##### 维度模型

是数仓工程领域最流行的建模经典， 从分析决策的需求出发构建模型， 重点关注用户如何快速地完成需求分析， 同时具有较好的大规模复杂查询的响应性能， 典型代表： 星形模型， 以及特殊场景下使用的雪花模型； 设计步骤如下：

1. 选择需要进行分析决策的业务过程
2. 选择粒度： 粒度是维度的一个组合；
3. 识别维表： 基于粒度设计维表，包括维度属性， 用于分析时进行分组和筛选；
4. 选择实时： 确定分析需要衡量的指标；

##### Data Vault模型

是ER模型的衍生, 设计出发点是为了实现数据的整合, 但不能直接用于数据分析决策;

强调建立可审计的基础数据层, 而不要求对数据进行过度的一致性处理和整合; 由以下几部分组成:

1. **Hub**: 企业的核心业务实体
2. **Link**: 代表Hub之间的关系, 与ER模型最大的区别就是将关系作为独立的单元抽象,提升模型的扩展性
3. **Satellite**: 是Hub的详细描述能容, 一个Hub可以有多个Satellite;

##### Anchor模型

对Data Vault模型做了进一步规范化处理, 由以下几部分组成:

1. **Anchors**: 代表业务实体, 且只有主键;
2. **Attributes**: 类似Data Vault的Satellite, 但更加规范化, 全部k-v结构化
3. **Ties**: 就是Anchors之间的关系
4. **Knots**: 代表那些可能会在多个Anchors中公用的属性的提炼

#### 7.5 阿里数据模型实践

1. 阶段1: 完全应用驱动的时代, 第一个数仓系统构建在Oracle上, 基本没有系统化的模型方法体系, 数据架构只有ODS+DSS两层;
2. 阶段2: 结合ER模型和维度模型,构建出四层的模型架构: ODL(操作数据层) + BDL(基础数据层) + IDL(接口数据层) + ADL(应用数据层);
3. 阶段3: 以维度建模为核心理念的模型方法论, 并进行了一定的升级和拓展, 构建了阿里的公共层模型数据架构体系;

阿里数据公共层建设的指导方法是一套统一化的集团数据整合及管理的方法体系(OneData), 包括一致性的指标定义体系,模型设计方法体系及配套工具

### 8. 阿里数据整合及管理体系

OneData是阿里内部进行数据整合及管理的方法体系和工具, 建设方法论的核心是: 从业务架构设计到模型设计,从数据研发到数据服务,做到数据可管理,可追溯,可规避重复建设;

**体系架构:**

<img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210409164652495.png" alt="image-20210409164652495" style="zoom:67%;" />

#### 8.1 规范定义

规范定义指以维度建模作为理论基础,构建总线矩阵,划分和定义数据域/业务过程/维度/度量/修饰类型/修饰词/时间周期/派生指标;

<img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210409164854432.png" alt="image-20210409164854432" style="zoom: 67%;" />

##### 8.1.1 术语解释

- **数据域**: 面向业务分析, 将业务过程或维度进行抽象的集合;
- **业务过程**: 指企业的业务活动事件, 是不可拆分的行为事件;
- **时间周期**: 用来明确数据统计的时间范围或者时间点;
- **修饰类型**: 对修饰词的一种抽象划分
- **修饰词**: 除了统计维度以外指标的业务场景限定抽象
- **度量/原子指标**: 基于某一业务事件行为下的度量,是业务定义中不可再拆分的指标, 具有明确业务含义的名词
- **维度**: 度量的环境,用来反映业务的一类属性;
- **维度属性**: 维度属性隶属于一个维度
- **派生指标**: =一个原子指标+多个修饰词(可选)+时间周期, 可以理解为对于原子指标业务统计范围的圈定;

##### 8.1.2 指标体系

**基本原则**:

1. 组成体系之间的关系

   - 派生指标有原子指标, 时间周期修饰词, 若干其他修饰词组合得到
   - 原子指标, 修饰词类型及修饰词, 直接归属在业务过程下,其中修饰词继承修饰类型的数据域
   - 派生指标可以选择多个修饰词,修饰词之间的关系为"或"或者"且"
   - 派生指标唯一归属一个原子指标,继承原子指标的数据域
   - 原子指标有确定的英文字段名,数据类型和算法说明, 派生指标要继承原子指标的英文名, 数据类型和算法要求;

2. 命名约定

   - 命名所用术语, 指标命名, 尽量使用英文简写, 其次是英文, 然后是汉语拼音首字母;

   - 业务过程: 英文名用英文或英文缩写或者中文拼音简写, 中文名用具体的业务过程中文

   - 原子指标: 中英文名均为动作+度量

   - 修饰词: 只有时间周期才会有英文名,且长度为2位, 其他修饰词无英文名

     <img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210409170935300.png" alt="image-20210409170935300" style="zoom: 50%;" />

   - 派生指标: 英文名: 原子指标英文名+时间周期修饰词(三位如'\_1d')+序号(4位如'\_001'); 中文名: 时间修饰词+[其他修饰词]+原子指标

**操作细则**

1. 派生指标的种类
   - 事务型指标: 对业务活动进行衡量的指标;
   - 存量型指标: 对实体对象某些状态的统计;
   - 复合型指标: 在事务型指标和存量型指标基础上复合而成的;
2. 复合型指标的类型
   - 比率型: 创建原子指标, 如CTR,满意率等, 示例"最近1天店铺首页CTR"
   - 比例型: 创建原子指标, 如百分比,占比等, 示例"最近1天无线支付金额占比"
   - 变化量型: 不创建原子指标, 增加修饰词, 在此基础上创建派生指标, 示例"最近1天订单支付金额上1天变化量"
   - 变化率型: 创建原子指标, 示例"最近7天海外买家字符金额上7天变化率";
   - 统计型: 不创建原子指标,在修饰类型"统计方法"下增加修饰词, 示例"自然月日均UV"
   - 排名型: 创建原子指标, 修饰词常用有统计方法(升序降序), 排名名次, 排名范围, 根据什么排序
3. 其他规则
   - 上下层派生指标同时存在时, 建议使用前者
   - 父子关系原子指标存在时, 派生指标使用子原子指标创建派生指标;

#### 8.2 模型设计

##### 8.2.1 模型层次

- **操作数据层(ODS)**: 把操作系统数据极狐无处理地存放在数据仓库系统中
- **公共维度模型层(CDM)**: 存放明细实时数据, 维表数据集公共指标汇总数据;
  - DWD层: 明细数据层
  - DWS层: 汇总数据层
- **应用数据层(ADS)**: 存放数据产品个性化的统计指标数据

<img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210409172950309.png" alt="image-20210409172950309" style="zoom:50%;" />

数据调用服务优先使用CDM数据, 当CDM没有数据时, 需评估是否需要创建CDM层数据, 如不需要, 才可直接调用ODS层数据. ADS层作为产品特有的个性化数据一般不对外提供数据服务;

##### 8.2.2 基本原则

1. **高内聚和低耦合**
   - 将业务相近或者相关,粒度相同的数据设计为一个逻辑或者物理模型
   - 将高概率同时访问的数据放在一起, 将低概率同时访问的数据分开存储
2. **核心模型与扩张模型分离**: 扩张模型字段支持个性化, 不能过度侵入核心模型, 以免破坏核心模型的架构简洁性与可维护性
3. **公共处理逻辑下沉及单一**: 越底层共用的处理逻辑应该在数据调度依赖的底层进行封装与实现, 不要让公共逻辑多处同时存在
4. **成本与性能平衡**: 适当数据冗余可换取查询和刷新性能, 不宜过度冗余和数据复制
5. **数据可回滚**
6. **一致性**
7. **命名清洗,可理解**

#### 8.3 模型实施

##### 8.3.1 业界常用模型实施过程

- **Kimball模型实施过程:** 主要探讨需求分析, 高层模型, 详细模型和模型审查
  1. 高层设计时期
  2. 详细模型设计舍弃
  3. 模型审查,再设计及验证
  4. 完成详细设计文档, 提交ETL设计和开发
- **Inmon模型实施过程**: 将模型划分为ERD层(实体关系层), DIS层(数据项集)和物理层, 采用螺旋式开发方法, 采用迭代方式完成多次需求
  1. ERD层是数据模型的最高层, 描述了业务中实体或主题域以及他们之间的关系;
  2. DIS层是中间层, 描述了关键字, 属性以及细节数据之间的关系;
  3. 物理层是数据建模的最底层, 描述了数据模型的物理特性;

##### 8.3.2 OneData建模实施过程

1. 数据调研: 业务调研+需求调研;

2. 架构设计:

   - 数据域划分: 数据域需要抽象提炼, 并且长期维护和更新, 但不轻易变动; 划分数据域时既能涵盖当前业务需求, 又能在新业务进入时无影响被包含进已有数据域或扩展新的数据域;

   - 构建总线矩阵: 明确每个数据域下的业务过程, 以及业务过程与哪些维度相关

     <img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210412093048739.png" alt="image-20210412093048739" style="zoom:50%;" />

3. 规范定义: 定义指标体系, 包括原子指标, 修饰词, 时间周期和派生指标;

4. 模型设计: 维度及属性的规范定义, 维表,明细事实表和汇总事实表的模型设计;

5. 总结

### 9. 维度设计

#### 9.1 维度设计基础

##### 基本概念

- 维度是维度建模的基础和灵魂; 是用于分析事实所需要的多样环境;
- 维度所包含的表示维度的列, 称为维度属性;
- 维度使用主键标识其唯一性, 主键也是确保与之相连的任何事实表之间存在引用完整性的基础, 分为代理键和自然键; 代理键是不具有业务含义的键, 一般用于处理缓慢变化维, 自然键是具有业务含义的键;

##### 维度基本设计方法

1. 选择维度或新建维度
2. 确定主维表(一般是ODS表);
3. 确定相关维度
4. 确定维度属性
   - 尽可能生成丰富的维度属性, 为数据统计分析探查提供良好基础
   - 尽可能多给出包括一些富有意义的文字性描述;
   - 区分数值型属性和事实: 如果字段通常用于查询约束条件或分组统计, 则作为维度属性; 如果通常用于参与度量的计算, 则作为事实;
   - 尽量沉淀出通用的维度属性

##### 维度的层次结构

层次的最底层代表维度中描述最低级别的详细信息, 最高层代表最高级别的概要信息;

##### 规范化和反规范化

当属性层次被实例化为一系列维度, 而不是单一的维度时, 被称为雪花模式; 大多数OLTP(联机事务处理系统)的底层数据结构在设计时采用此种规范化数据, 可有效避免数据冗余导致的不一致性;

将维度的属性层合并到单个维度中的操作成为反规范化, 可避免分析过程中的关联操作, 更为方便, 易用且性能好;

采用雪花模式除了解约一部分存储外, 对OLAP系统没有其他效用, 而现阶段存储的成本非常低,  出于易用性和性能的考虑, 维表一般是很不规范化的, 也就是使用维表的空间来换取简明性和查询性能;

##### 一致性维度和交叉查探

Kimball的数据仓库总线架构提供了一种分解企业级数据仓库规划任务的合理方法,通过构建企业范围内一致性维度和事实来构建总线架构;

将不同数据域的商品的事实合并在一起进行数据查探, 称为交叉查探;

维度一致性的几种表现形式:

1. 共享维表
2. 一致性上卷, 其中一个维度的维度属性是另一个维度的维度属性的子集, 且两个维度的公共维度属性结构和内容相同;
3. 交叉属性, 两个维度具有部分相同的维度属性;

#### 9.2 维度设计高级主题

##### 维度整合

数仓的数据来源是大量的分散的面向应用的操作型环境, 这些数据进入数仓后, 需要进行数据集成;

将面向应用的数据转换为面向主题的数仓数据, 本身就是一种集成, 体现在如下方面:

- 命名规范的统一
- 字段类型的统一
- 公共代码及代码值的统一;
- 业务含义相同的表的统一: 主要依据高内聚,低耦合的理念, 通常有如下几种集成方式:
  - 采用主从表的设计方式;
  - 直接合并, 共有信息和个性信息都放在用同一个表中;
  - 不合并;

维表整合与上述方面类似, 表级别整合有两种表现形式:

- 垂直整合, 即不同的来源表包含相同的数据集, 只是存储的信息不同;
- 水平整合, 即不同的来源表包含不同的数据集, 不同子集之间无交叉,也可存在部分交叉;

##### 水平拆分

维度设计过程中需要考虑以下原则:

1. 扩展性: 当源系统,业务逻辑变化时, 能通过较少的成本快速扩展模型, 保持核心模型的相对稳定性;
2. 效能: 在性能和成本方面取得平衡;
3. 易用性: 模型可理解性高, 访问复杂度低;

对维度进行水平拆分时, 考虑如下两个依据:

1. 维度的不同分类的属性差异情况, 当维度属性随类型变化较大时, 没必要把所有属性建立在一个表中, 此时应将维度的不同分类实例化为不同的维度, 同时主维度中保存公共属性;
2. 依据业务的关联程度, 两个相关性较低的业务, 耦合在一起弊大于利, 对模型的稳定性和易用性影响较大;

##### 垂直拆分

出于扩展性,产出时间,易用性等方面的考虑, 设计主从维度. 主维表存放稳定,产出时间早,热度高的属性; 从维表存放变化较快,产出时间晚,热度低的属性;

##### 历史归档

在数仓中, 可借用前台数据库的的归档策略, 定期将历史数据归档至历史维表, 数仓归档策略有如下几种方式:

1. 同前台归档策略, 在数仓中实现前台归档算法, 定期对历史数据进行归档, 存在以下问题
   - 前台归档策略复杂, 实现成本高
   - 前台归档策略可能经常变化, 维护和沟通成本较高
2. 同前台归档策略, 但采用数据库变更日志的方式: 通过增量日志的删除标志, 作为前台数据归档的标志, 通过此标志对数据仓库的数据进行归档;
3. 数据仓库自定义归档策略, 原则是尽量比前台应用晚归档,少归档

建议使用归档策略2

#### 9.3 维度变化

##### 缓慢变化维

维度的属性并不是静态的, 会随着时间的流逝发生缓慢的变化, 与数据增长较为快速的事实表相比,维度变化相对缓慢;

Kimball理论中三种处理缓慢变化维的方式:

1. 重写维度值, 不保留历史数据, 始终取最新数据;
2. 插入行的维度行, 保留历史数据, 维度值变化前的事实和过去的维度值关联, 纬度值变化后的事实和当前的纬度值关联;
3. 添加维度列

具体选择哪种处理方式需要根据业务需求来进行选择;

##### 快照维表

在Kimball维度建模中, 必须使用代理键作为每个维表的主键, 用于处理缓慢变化维, 但是阿里在数仓建设实践过程中, 虽然使用了Kimball维度建模理论,但实际并未使用代理键:

1. 阿里的数据量庞大, 使用的是分布式计算平台MaxCompute, 每个表的记录生成稳定的全局唯一的代理键难度很大
2. 使用代理键会大大增加ETL的复杂性, 对ETL任务的开发和维护成本很高

阿里使用快照的方式, 在不使用代理键的情况下处理缓慢变化维的问题, 每天保留一份全量快照数据:

- 优点: 简单有效,开发维护成本低; 使用方便, 理解性好
- 弊端: 存储的极大浪费

但是由于现在的存储成本远低于CPU, 内存等的成本, 此方法弊大于利;

##### 极限存储

历史拉链存储: 通过行政两个时间戳字段(start_dt和end_dt), 将所有以天为粒度的变更数据都记录下来;

此种存储存在理解障碍, 同时随着时间的推移,分区数量会极度膨胀; 因此阿里使用极限存储的方式来处理:

1. **透明化**: 底层的数据还是历史拉链存储, 但在上层做一个视图操作, 通过分析语句的语法树, 把对极限存储前的表的查询转换为对极限存储表的查询;
2. **分月做历史拉链表**: 每年最多可能产生的分区数从66430降低到5232

采用极限存储, 极大压缩了全量存储的成本, 又可以达到对下游用户透明的效果, 但本身也有一定局限性:

1. 产出效率低
2. 对于变化频率高的数据并不能达到节约成本的效果;

因此在实际生产中会做一些额外处理:

1. 在做极限存储前有一个全量存储表,历史数据通过映射的方式关联到极限存储表
2. 对于部分变化频繁的字段进行过滤;

##### 微型维度

采用极限存储, 需要避免维度的过度增长; 将变化频繁的维度从维表移出到新的维表中, 可以解决维度的过度增长导致极限存储效果大打折扣的问题, 一种解决方法是**垂直拆分**,另一个解决方法是采用**微型维度**;

微型维度的创建时通过将一部分不稳定的属性从主维度移出, 并将他们放置到拥有自己代理键的新表中来实现的;

但是阿里并未使用此技术, 主要有以下几点原因:

1. 微型维度的局限性: 微型维度是事先把所有可能值的组合加载的,需要考虑每个属性的基数, 且必须是枚举值
2. ETL逻辑复杂
3. 破坏了维度的可浏览性

#### 9.4 特殊维度

##### 递归层次

维度的递归层次, 按照层级是否固定分为均衡层次结构和非均衡层次结构, 如类目,地区等具有固定数量级别的递归层次为均衡层次结构;

很多数仓系统和工具不支持递归SQL, 且用户使用递归SQL的成本较高, 因此在维度模型中需要对递归层次结构进行处理:

1. 层次结构扁平化: 通过建立维度的固定数量级别的属性来实现, 对于均衡层次结构, 采用扁平化最有效; 但存在以下问题
   - 上钻或下钻前必须知道所属类目的层级
   - 需要回填解决空值问题
   - 对于非平衡层次结构, 扩展性较差
2. 层次桥接表:  解决了层次结构扁平化带来的一些问题, 但是其加工及使用逻辑复杂,且带来双重计算的隐患, 在实际工作中需要根据具体的业务需求选择技术方案;

<img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210412143946176.png" alt="image-20210412143946176" style="zoom:50%;" />

##### 行为维度

根据加工方式, 行为维度可分为以下几种:

- 另一种维度的过去行为, 如买家最近一次访问淘宝的时间
- 快照事实行为维度, 如买家从年初截至当前的淘宝交易金额
- 分组事实行为维度, 将数值型事实转换为枚举值, 如买家信用等级
- 复杂逻辑事实行为维度, 通过复杂算法加工或多个事实综合加工得到, 如卖家主营类目

行为维度有两种处理方式: 冗余至现有维表中, 或加工成单独的维表, 需参考以下原则

1. 避免维度过快增长;
2. 避免耦合度过高;

##### 多值维度

事实表的一条记录在某维度表中有多条记录与之对应, 可根据业务的表现形式和统计分析需求进行处理方式选择:

1. 降低事实表的粒度
2. 采用多字段
3. 采用较为通用的桥接表

##### 多值属性

维表中的某个字段同时又多个值, 称之为"多值属性", 常见处理方式如下:

1. 保持维度主键不变, 将多值属性放在维度的一个属性字段中(如k-v形式);
2. 保持维度主键不变, 但将多值属性放在维度的多个属性字段中;
3. 维度主键发生变化, 一个维度值存放多条记录;

##### 杂项维度

杂项维度是由操作型系统中的指示符或者标志字段组合而成, 一般不在一致性维度之列;

### 10. 事实表设计

#### 10.1 事实表基础

##### 事实表特性

- 事实表中一条记录所表达的业务细节程度被称为粒度, 通常粒度可通过两种方式表达: 维度属性组合表示的细节程度, 一种是所表示的具体业务含义;
- 维度属性也可存储到事实表中,  存储到事实表中的维度列被称为"退化维度";
- 事实表有三种类型: **事务事实表, 周期快照事实表, 累积快照事实表;**

##### 事实表设计原则

1. 尽可能包含所有与业务过程相关的事实;
2. 只选择与业务过程相关的事实;
3. 分解不可加性事实为可加的组件: 如订单优惠率, 应该分解为订单原价与订单优惠价;
4. 在选择维度和事实之前必须先声明粒度: 粒度的声明式事实表设计中不可忽视的重要一步;
5. 在同一个事实表中不能有多种不同粒度的事实;
6. 事实的单位要保持一致;
7. 对事实的null值要处理: 建议用零值填充;
8. 使用退化维度提高事实表的易用性;

##### 事实表设计方法

1. **选择业务过程及确定事实表的类型;**
2. **声明粒度**: 应该尽量选择最细级别的原子粒度, 以确保事实表的应用具有最大的灵活性;
3. **确定维度:** 应该选择能够描述清楚业务过程所处的环境的维度信息;
4. **确定事实**: 事实有可加性,半可加性,非可加性三种类型, 需要将不可加性事实分解为可加的组件;
5. **冗余维度**;

#### 10.2 事务事实表

##### 设计过程

1. 选择业务过程: 为了便于进行独立的分析研究, 应该为每个业务过程建立一个事实表;
2. 确定粒度
3. 确定维度
4. 确定事实
5. 冗余维度

##### 单事务事实表

对每个业务过程设计一个事实表, 可以方便地对每个业务过程进行独立的分析研究;

##### 多事务事实表

将不同的事实放到同一个事实表中, 即同一个事实表包含不同的业务过程;

1. 不同业务过程的事实使用不同的事实字段进行存放(如淘宝交易事务事实表)
   - 当不同业务过程的度量差异较大时, 选择此种方式;
   - 存在问题: 度量字段零值较多;
2. 不同业务过程的事实使用同一个事实字段进行存放, 但增加一个业务过程标签;(如淘宝收藏商品事务事实表)
   - 当不同业务过程的度量比较相似, 差异不大时, 采用此种方式
   - 存在问题: 在同一个周期内会存在多条记录;

##### 两种事实表对比

1. **业务过程**: 单事实业务表一个表只反映一个业务过程的事实, 多事务事实表一个表反映多个业务过程;
2. **粒度与维度**: 当不同业务过程的粒度相同, 同时拥有相似的维度时, 可考虑采用多事务事实表; 粒度不同必定是不同的事实表;
3. **事实**: 单事务事实表在处理事实上比较方便和灵活; 如果单一业务过程的事实较多, 同时不同业务过程的事实又不相同, 可以考虑使用单事务事实表;
4. **下游业务使用**: 单事务事实表对于下游用户而言更容易理解;
5. **计算存储成本**: 当业务过程数据来源于同一个业务系统, 具有相同的粒度和维度, 且维度较多而事实相对不多时, 可以考虑使用多事务事实表, 不仅加工计算成本较低, 同时是存储上也相对节省;

##### 事实的设计准则

1. **事实完整性**: 事实表包含与其描述的过程有关的所有事实
2. **事实一致性**: 明确存储每一个事实以确保度量的一致性;
3. **事实可加性**;

#### 10.3 周期快照事实表

 快照事实表在确定的间隔内对实体的度量进行抽样, 这样可以很容易地研究实体的度量值, 而不需要聚集长期的事务历史;

##### 特性

1. 用快照采样状态
2. 快照粒度: 事务事实表的粒度可以通过业务过程中所涉及的细节程度来描述, 但快照事实表的粒度通常总是被多维声明;
3. 密度与稀疏性: 快照事实表和事务事实表的关键区别在密度上: 事务事实表是稀疏的, 而稠密性是快照事实表的重要特征;
4. 半可加性: 在快照事实表中收集到的状态度量都是半可加的

##### 实例

快照事实表的设计步骤可以归纳为:

- 确定快照事实表的快照粒度
- 确定快照事实表采样的状态度量

快照事实表的产出方式:

- 从事务事实表进行汇总产出
- 直接使用操作型系统的数据作为周期快照事实表的数据源进行加工;

##### 全量快照事实表

对于全量快照事实表, 在确定粒度及确定状态度量的基础上, 还增加了冗余维度;

##### 注意事项

1. 事务与快照成对设计: 既丰富了星形模型, 又降低了下游分析的成本;
2. 附加事实: 一般在设计周期快照事实表时会附加上一些上个采样周期的状态度量;
3. 周期到日期度量: 以满足更多的统计分析需求

#### 10.4 累积快照事实表

##### 设计过程

1. 选择业务过程
2. 确定粒度
3. 确定维度
4. 确定事实
5. 退化维度

##### 特点

1. 数据不断更新
2. 多业务过程日期: 累积快照事实表适用于具有较明确起止时间的短生命周期的实体;

##### 特殊处理

1. 非线性处理
   - 业务过程的统一;
   - 针对业务关键里程碑构建全面的流程;
   - 循环流程的处理;
2. 多源过程: 对于多元业务建模, 主要考虑事实表的粒度问题;
3. 业务过程取舍;

##### 物理实现

针对累计快照事实表模型设计, 其有不同的实现方式:

1. 全量表的形式, 一般为日期分区表, 适用于全量数据较少的情况;
2. 全量表的变化形式, 池中方式主要针对事实表数据量很大的情况;
3. 以业务实体的结束时间分区;

#### 10.5 三种事实表比较

<img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210412172821964.png" alt="image-20210412172821964" style="zoom: 67%;" />

- **事务事实表**记录的是事务层面的事实, 用于跟踪业务过程的行为, 保存的是最原汁的数据; 更新方式为增量更新
- **周期快照事实表**以具有规律性的,可预见的时间间隔来记录事实, 记录的事实是这个时间段内一些聚集事实值或状态度量, 其更新方式为增量更新;
- **累计快照事实表**被用来跟踪实体的一系列业务过程的进展情况, 通常有多个日期字段, 在数据加载完成后, 可以对数据进行更新;

#### 10.6 无事实的事实表

常见的无事实的事实表主要有如下两种:

1. 事件类的, 记录事件的发生, 如日志类事实表;
2. 条件,范围或资格类的, 记录维度与维度多对多之间的关系, 如客户和销售人员的分配情况;

#### 10.7 聚集型事务表

数据仓库的性能是数仓建设是否成功的重要标准之一; 聚集主要是通过汇总明细粒度数据来获得改进查询性能的效果;

##### 聚集的基本原则

1. **一致性**: 最简单的方法是确保聚集星形模型中的维度和度量与原始模型中的维度和度量保持一致;
2. **避免单一表设计**: 不要在同一个表中存储不同层次的聚集数据;
3. **聚集粒度可不同**: 聚集并不需要保持与原始明细粒度数据一样的粒度, 聚集只关心所需要查询的维度;

##### 聚集的基本步骤

1. 确定聚集的维度
2. 确定一致性上钻
3. 确定聚集事实

##### 阿里公共汇总层

1. 基本原则
   - 数据共用性
   - 不跨数据域
   - 区分统计周期
2. 交易汇总表设计
   - 按商品粒度汇总
   - 按卖家粒度汇总
   - 按卖家,买家,商品粒度汇总
   - 按二级类目汇总

##### 聚集补充说明

1. 聚集是不跨越事实的: 聚集的维度和度量必须与原始模型保持一致;
2. 聚集带来的问题: 聚集会带来查询性能的提升, 但也会增加ETL维护的难度;

## 数据管理篇

### 11. 元数据

#### 11.1 元数据概述

##### 元数据定义

元数据是关于数据的数据, 主要记录数据仓库中模型的定义, 各层级间的映射关系, 监控数据仓库的数据状态及ETL的任务运行状态;

根据用途的不同可分为: 技术元数据和业务元数据

1. 技术元数据是存储关于数据仓库系统技术细节的数据, 阿里常见的技术原数据有:
   - 分布式计算系统存储元数据, 如MaxCompute表/列/分区等信息;
   - 分布式计算系统运行元数据, 如所有作业运行等信息;
   - 数据开发平台中数据同步, 计算任务, 任务调动等信息;
   - 数据质量和运维相关元数据;
2. 业务元数据从业务角度描述数仓中的数据, 常见的业务元数据有:
   - OneData元数据, 如维度及属性,业务过程,指标等的规范化定义;
   - 数据应用元数据;

##### 元数据价值

元数据是数据管理, 数据内容, 数据应用的基础, 在数据管理方面为集团数据提供在计算,存储,成本,质量,安全,模型等治理领域上的数据支持;

##### 统一元数据体系建设

1. 梳理清楚元仓底层数据域, 对元数据做分类;
2. 丰富表和字段使用说明, 方便使用和理解
3. 根据元仓底层数据构建元仓中间层, 依据OneData规范, 建设元数据基础宽表, 也就是元数据中间层;

#### 11.2 元数据应用

数据的真正价值在于数据驱动决策, 通过数据直到运营;

##### Data Profile

核心思路是为纷繁复杂的数据建立一个脉络清晰的血缘图谱; Data Profile实际承担的是为元数据"画像"的任务;

Data Profile共有四类标签:

1. **基础标签**: 针对数据的存储情况,访问情况,安全等级等进行打标;
2. **数仓标签**: 针对数据时增量还是全量,是否可再生,数据的生命周期来进行标签化处理;
3. **业务标签**: 根据数据归属的主题域,产品线,业务类型为数据打上不同的标签;
4. **潜在标签**: 这类标签主要是为了说明数据潜在的应用场景;

##### 元数据门户

阿里基于元数据产出的最重要的产品是元数据门户, 致力打造一站式的数据管理平台,高效的一体化数据市场; 包括"前台"和"后台", 前台产品为数据地图, 后台产品为数据管理

- 数据地图围绕数据搜索, 服务于数据分析,数据开发,数据挖掘,算法工程师,数据运营等数据表的使用者和拥有者, 提供快捷的数据搜索服务;
- 数据管理平台围绕数据管理,服务于个人开发者,BU管理者,系统管理员等用户, 提供个人和BU全局资产管理,成本管理和质量管理等;

##### 应用链路分析

通过应用链路分析, 产出表级学员,字段血缘和表的应用血缘, 其中标表级学员有两种计算方式: 一是通过MaxCompute任务日志进行解析; 二是根据任务以来进行解析;

常见的应用链路分析应用主要有影响分析, 重要性分析,下线分析,链路分析,寻根溯源,故障排查等;

##### 数据建模

传统的数仓建模一般采用经验建模的方式, 效率较低且不准确; 而通过元数据驱动的数仓建模, 可提高数仓建模的数据化指导, 提升建模效率;

所使用的元数据主要有:

- 表的基础元数据, 包括下游情况,查询次数,关联次数等
- 表的关联关系元数据, 包括关联表,关联类型,关联字段等
- 表的字段的基础元数据, 包括字段名称,字段注释,查询次数等;

在星型模型设计过程中, 可能类似于如下使用元数据:

- 基于下游使用中管理次数大于某个阈值的表或查询次数大于某个阈值的表等元数据信息, 筛选用于数据模型建设的表;
- 基于表的字段元数据, 如字段中的时间字段等;
- 基于主从表的关联关系,关联次数, 确定和主表关联的从表;
- 基于主从表的字段使用情况, 如字段的查询次数,过滤次数等;

##### 驱动ETL开发

通过元数据, 直到ETL工作, 提高ETL的效率; 如通过元数据驱动一键,批量高效数据同步的OneClick; OneClick覆盖的另一场景是存量数据日常维护;

### 12. 计算管理

#### 12.1 系统优化

Hadoop等分布式计算系统评估资源的方式, 一般书根据输入数据量进行静态评估; 对于Reduce任务, 其输入来自于Map的输出, 一般只能根据Map任务的输入进行评估, 经常和实际需要的资源数相差很大, 因此在任务稳定的情况下, 可以考虑基于任务的历史执行情况进行资源评估, 即HBO(基于历史的优化器);

CBO(基于代价的优化器), MaxCompute采用各种抽样统计算法, 通过较少的资源获得大量的统计信息, 基于先进的优化模型, 具备了晚上的CBO能力;

##### HBO

HBO是根据任务历史执行情况为任务分配更合理的资源; HBO是对集群资源分配的一种优化: 任务执行历史+集群状态信息+优化规则->更优的执行配置;

在默认的Instance算法下, 小任务存在资源浪费, 而大任务却资源不足, 因此HBO应运而生; HBO一般通过自适应调整系统参数来达到控制计算资源的目的;

**HBO原理**

- 前提: 最贱7天内任务代码没有翻身变更且任务运行4次
- Instance分配逻辑: 基础资源估算值 + 加权资源估算值
  1. 基础资源数量的逻辑
     - 对于Map Task, 根据期望的每个Map能处理的数据量, 再结合用户提交任务的输入数据量, 估算任务所需要的Map数量;
     - 对于Reduce Task, 比较Hive使用Map输入数据量, MaxCompute使用最近7天平均输入数据量作为Reduce的输入数据量, 用于计算Instance的数量;
  2. 加权资源数量的逻辑
     - 对于Map Task, 系统需要初始化期望而定每个Map能处理的数据量
     - 对于Reduce Task, 方法同上;
- CPU/内存分配逻辑: 类似于Instance分配逻辑;

**HBO效果**

1. 提高CPU利用率
2. 提高内存利用率
3. 提高Instance并发数
4. 降低执行时长

**HBO改进和优化**

为应对数据量暴涨的情况, HBO增加了根据数据量动态调整Instance数的功能, 主要依据Map的数据量增长情况进行调整;

##### CBO

MaxCompute 2.0 引入了基于代价的优化器(CBO), 根据手机的统计信息来计算每种执行方式的代价, 进而选择最优的执行方式; 通过性能评测, MaxCompute 2.0离线计算比同类产品Hive 2.0 on Tez快90%以上;

1. **优化器原理**

   优化器引入了Volcano模型, 有多个模块相互组合协调工作, 包括Meta Manager(元数据),Statistics(统计信息),Rule Set(优化规则集),Volcano Planner Core(核心计划器)

   - Meta Manager: Meta模块主要提供元数据信息, 对于Meta的管理, MacCompute提供了Meta Manager与优化器进行交互;
   - Statistics: 主要是帮助优化器选择几乎是,提供准确的统计信息;
   - Rule Set: 优化规则是根据不同情况选择不同的优化点, 然后由优化器根据代价模型来选择启用哪些优化规则;
   - Volcano Planner Core: 是整个优化器的灵魂, 会将所有信息统一起来处理, 然后根据代价模型的计算, 获得一个最优计划;
     1. 代价模型: 代价模型会根据不同操作符计算不同的代价,然后计算出整个计划中最小代价的计划
     2. 工作原理: 假设Planner的输入是一棵由Compiler解析好的计划树, 简称RelNode树, 每个节点简称RelNode
        - Volcanno Planner创建
        - Planner优化
          - 规则匹配: 是指RelNode满足规则的优化条件而建立的一种匹配关系;
          - 规则应用: 是指从规则队列中弹出一个已经匹配成功的规则进行优化;
          - 代价计算: 代价计算由代价模型对每个RelNode的代价进行估算;
   
2. **优化器新特性**

   - 重新排序Join

     Join的性能直接关系到整个SQL的性能, 重新排序Join可以认为是将Join的所有不同输入进行一个全排列, 然后找到代价最小的一个排列;

   - 自动MapJoin

     Join实现算法有多种, 目前主要有Merge Join和MapJoin. 对于小数据量, MapJoin比Merge Join心更难更优; 自动MapJoin充分利用优化器的代价模型进行估算, 获得更优的MapJoin方式, 而不是通过Hint方式来进行处理;

3. **优化器使用**

   如果用户需要使用哪些优化规则, 只要将规则的缩写名称加入白名单即可; 反之需要关闭哪些优化规则, 只要将名称加入黑名单即可;

#### 12.2 任务优化

SQL/MR 作业一般会生成MapReduce任务, 在MaxCompute中则会生成MaxCompute Instance, 通过唯一ID进行标识:

- Fuxi Job: 对于MaxCompute Instance, 则会生成一个或多个有Fuxi Task组成的有向无环图, 即Fuxi Job;
- Fuxi Task: 主要包含三种类型, 分别是Map, Reduce和Join;
- Fuxi Instance: 真正的计算单元, 一般和槽位(slot)对应;

##### Map倾斜

**背景**

Map端是MR任务的起始阶段, Map端的主要功能是从磁盘中间数据读入内存, 两个主要过程如下:

- 每个输入分片会让一个Map Instance来处理, 在默认情况下, 以Pangu文件系统的一个文件块大小(默认256MB)为一个分片;
- 在写入磁盘前, 线程首先根据Reduce Instance的个数划分分区, 数据将会给句Key值Hash到不同的分区上, 一个Reduce Instance对应一个分区的数据;

在Map端读数据时, 由于读入数据的文件大小分布不均匀, 因此会导致有些Map Instance读取并处理的数据特别多, 而有些Map Instance处理的数据特别少, 造成Map端长尾; 以下两种情况可能会导致Map端长尾:

- 上游表文件的大小特别不均匀, 并且小文件特别多;
- Map端做聚合时, 由于某些Map Instance读取文件的某个值特别多而引起长尾, 主要是指Count Distinct操作

**方案**

第一个原因造成的Map端长尾, 可通过对上游合并小文件, 同时调节本节点的小文件的参数来进行优化;

第二个原因造成的Map端长尾, 可通过"distribute by rand()", 将Map端分发后的数据重新按照随机值在进行一次分发;

##### Join倾斜

**背景**

MaxCompute SQL在Join执行阶段会将Join Key相同的数据分发到同一个执行Instance上处理, 如果某个Key上的数据量比较大, 则会大致该Instance执行时间较长. 常见倾斜场景如下:

- Join的某路输入比较小, 可以采用MapJoin, 避免分发引起长尾;
- Join的每路输入都较大, 且长尾是空值导致的, 可以将空值处理层随机值, 避免聚集
- Join的每路输入都较大, 且长尾是热点值导致的, 可以对热点值和非热电子分别进行处理, 再合并数据;

**方案**

1. MapJoin方案
   - MapJoin的原理是将Join操作提前到Map端执行, 将小表读入内存, 顺序扫描大表完成Join
   - MapJoin使用方法: 在代码select后机上"/*+mapjoin(a) */"即可, 其中a代表小表的别名; MaxCompute可以自动选择是否使用MapJOIN, 可以不用显示Hint
   - MapJoin对小表的大小有限制, 默认小表读入内存后大小不能超过512MB, 可设置加大内存, 最大2048MB;
2. Join因空值导致长尾
   - 将空值处理成随机值
3. Join因为热点值导致长尾
   - 先将热点key去除, 对于主表数据用热点key切分成热点数据和非热点数据分别处理, 最后合并;

##### Reduce倾斜

**背景**

Reduce端负责的是对Map端梳理后的有序key-value键值对进行聚合, 得到最终聚合的结果;

MaxCompute中Distinct的执行原理是将需要去重的字段以及Group By字段临河作为key将数据分发到Reduce端; 因为Distinct操纵, 数据无法在Map端的Shuffle阶段根据Group By先做一次聚合操作, 以减少传输的数据量, 而是将所有的数据都传输到Reduce端, 但key的数据分发不均匀时, 就会导致Reduce端长尾;

Reduce端产生长尾的主要原因就是key的数据分布不均匀, 如下情况会造成Reduce端长尾:

- 对同一个表按照维度对不同的列进行Count Distinct操纵, 造成Map端数据膨胀, 从而使得下游的Join和Reduce出现链路上的长尾;
- Map端直接做聚合时出现key值分布不均匀, 造成Reduce端长尾;
- 动态分区数过多时可能造成小文件过多, 从而引起Reduce端长尾;
- 多个Distinct同时出现在一段SQL代码中时, 数据会被分发多次, 不仅会造成数据膨胀N倍, 还会把长尾现象放大N倍;

**方案**

- 对于第二种情况, 可以对热点key进行单独处理,然后通过Union All合并
- 对于第三种情况, 可以把符合不同条件的数据放到不同的分区, 避免通过多次"Insert Overwrite"写入表中;
- 对于第四种情况, 通过修改代码避免多个Distinct同时出现, 如SUM(字段A > 0 then 1 else 0 end);

目前Reduce端数据倾斜很多是由于Count Distinct问题引起的, 因此在ETL开发工作中应该予以总是Count Distinct问题, 避免数据膨胀;

### 13. 存储和成本管理

#### 13.1 数据压缩

在分布式文件系统中, 为了提高数据的可用性与性能, 通常会将数据存储3份; 目前MaxCompute中提供了archive压缩方法, 采用了更高压缩比的压缩算法, 可以进数据保存为RAID file的形式, 大约能节省一半的物理空间;

#### 13.2 数据重分布

在MaxCompute中主要采用基于列存储的方式, 由于每个表的数据分布不同, 插入数据的顺序不一样, 会导致压缩效果有很大的差异, 因此通过修改表的数据重分布, 避免列热点, 将会节省一定的存储空间;

#### 13.3 存储治理项优化

在元数据的基础上, 诊断,加工成多个存储治理优化项, 通过对优化项的数据诊断, 形成治理项, 通过流程的方式进行运转管理, 最终实现存储管理优化;

#### 13.4 生命周期管理

##### 生命周期管理策略

1. **周期性删除策略**: 存储的数据都有一定的有效期, 从数据创建开始到过时, 可以周期性删除数据;
2. **彻底删除策略**: 无用表数据或者ETL过程产生的临时数据, 以及不需保留的数据, 进行及时删除;
3. **永久保留策略**: 重要且不可恢复的底层数据和引用数据需要永久保留;
4. **极限存储策略**: 可以超高压缩重复镜像数据, 通过平台化配置手段实现透明访问;
5. **冷数据管理策略**: 是永久保留策略的扩展, 永久保留的数据需要迁移到冷数据中心进行永久保存;
6. **增量表merge全量策略**: 对于某些特定的数据, 需要改成增量同步与全量merge的方式, 对于对应的delta增量表的保留策略, 目前默认93天;

##### 通过生命周期管理矩阵

1. **历史数据等级划分**
   - P0: 非常很重要的主题域数据和应用数据, 具有不可恢复性;
   - P1: 重要的业务数据和应用数据, 具有不可恢复性;
   - P2: 重要的业务数据和应用数据, 具有可恢复性;
   - P3: 不重要的业务数和应用数据, 具有可恢复性
2. **表类型划分**
   - 事件型流水表(增量表): 无重复或无主键数据, 如日志
   - 事件型镜像表(增量表): 业务过程性数据, 有主键, 属性会缓慢变化, 如交易,订单
   - 维表: 包括维度与维度属性数据, 如用户表,商品表;
   - Merge全量表:  包括业务过程性数据或者维表数据;
   - ETL临时表: ETL处理过程中产生的临时表数据, 最多保留7天;
   - TT临时数据: TT拉取的数据和DbSync产生的临时数据最终会流转到ODS层;
   - 普通全量表

<img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210413152454903.png" alt="image-20210413152454903" style="zoom:67%;" />

<img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210413152514539.png" alt="image-20210413152514539" style="zoom:67%;" />

#### 13.5 数据成本计量

存储成本(消耗的存储资源)+计算成本(计算过程中的CPU消耗)+扫描成本(对上游数据表的扫描)

### 14. 数据质量

数据质量是数据分析结论有效性和准确性的基础

#### 14.1 数据质量保障原则

阿里对于数仓主要从四个方面进行评估: 完整性, 准确性, 一致性和及时性;

1. **完整性:** 数据的记录和信息是否完整, 是否存在缺失的情况;
2. **准确性**: 数据中心记录的信息和数据是否准确, 是否存在异常或者错误的信息;
3. **一致性**: 一般体现在跨度很大的数仓体系中, 内部有很多业务数仓分支, 对于同一份数据, 必须保持一致性;
4. **及时性**: 保障数据能够及时产出, 从而体现数据的价值

#### 14.2 数据质量方法描述

1. **消费场景知晓**: 通过数据资产登记和基于元数据的应用链路分析解决消费场景知晓的问题;
   - **数据资产等级**:
     1. **毁灭性质A1**: 数据一旦出错将引起重大资产损失, 造成重大公关风险;
     2. **全局性质A2**: 数据直接或间接用于集团级业务和效果的评估, 重要平台的运维等;
     3. **局部性质A3**: 数据直接或间接用于内部一般数据产品或者运营/产品报告;
     4. **一般性质A4**: 数据主要用于小二的日常数据分析, 出现问题基本不会带来影响或影响极小
     5. **未知性质Ax**: 不能明确说出数据的应用场景, 则标注为未知;
   - 数据资产等级落地方法: 借助元数据可以了解到数仓中哪些表服务于某个数据产品, 根据数据产品的资产等级,  再依托于元数据的上下游血缘, 就可以将整个消费链路打上资产等级标签;
2. **数据生产加工各环节卡点校验**: 包括在线系统和离线系统数据生产加工各个环节的卡点校验;
   - 在线系统卡点校验: 指在在线业务系统的数据生成过程中进行的卡点校验; 如何保障在线数据和离线数据的一致性, 需要工具和人员双管齐下
     - 工具: 发布平台(集成通知功能,针对重要的场景进行发布进行卡点)+数据库表的变化感知
     - 人员: 提高在线开发人员的意识, 使其意识到数据的重要性, 做到业务端和数据端一致;
   - 离线系统卡点校验: 如何保障数据加工过程中的质量, 是数仓数据质量保障的重要环节;
     - 代码提交时的卡点校验-SQLSCAN
     - 任务发布上线时的卡点校验-Code Review + 回归测试 + Dry Run测试
     - 节点变更或数据重刷前的变更通知
3. **风险点监控**: 针对在数据日常运行过程中可能出现的数据质量和时效等问题进行监控;
   - 在线数据风险点监控 - **实时业务检测平台BCP**
     1. 用户在BCP平台进行数据源订阅, 以获取需要校验的数据源
     2. 针对订阅的数据源进行规则的编写
     3. 配置警告, 针对不同的规则配置不同的告警形式
   - 离线数据风险点监控: 任务优先级 + 任务报警 + **摩萨德**(离线任务的监控报警系统)
     - 数据准确性 - 阿里使用**DQC**来保障数据准确性
     - 数据及时性
4. **质量衡量**: 既有事前衡量, 如DQC覆盖率, 也有时候衡量, 主要用于跟进质量问题, 并用于数据质量的复盘;
   - **数据质量起夜率**: 每月开发人员起夜处理次数, 频繁起夜说明数据质量的建设不够完善;
   - **数据质量事件**: 每一个数据质量问题,都记录一个数据质量事件;
   - **数据质量故障体系**: 严重的数据质量事件, 升级为故障(给公司带来资产损失或者公关风险)
     - 故障定义
     - 故障等级(p1-p4)
     - 故障处理
     - 故障Review
5. **质量配套工具**

## 数据应用篇

### 15. 数据应用

#### 15.1 生意参谋

##### 功能架构和技术能力

- 看我情: 数据主要基于店铺经营全链路的各个环节进行设计
- 看行情: 通过市场行情, 为商家提供行业维度的大盘分析
- 看敌情: 在保障商家隐私和数据安全的前提下提供竞争分析, 涉及其他商家核心数据的, 均作指数化处理

体验方面, 生意参谋的数据来自阿里大数据公共层OneData, 避免了数据指标定义不一致;

技术层面, 数据中台的实时数据计算技术也可保障生意参谋众多数据指标的实时性和准确性;

用户洞察方面, 基于阿里大数据团队全力打造OneID体系;

#### 15.2 对内数据产品平台

##### 定位

对于企业内部数据产品, 用户是公司的员工, 解决的痛点是用户对业务发展中的数据监控,问题分析,机会洞察,决策支持等诉求, 提供给用户高效率获取数据,合理分析框架,数据负责业务决策的价值;

##### 产品建设历程

1. 临时需求阶段
2. V2.0 自动化报表阶段
3. 自主研发BI工具阶段: "快门"+"小站"
4. V4.0 数据产品平台

##### 整体架构介绍

阿里数据平台, 共有四个层次: 数据监控,专题分析,应用分析,数据决策

<img src="C:\Users\buer\AppData\Roaming\Typora\typora-user-images\image-20210414100722012.png" alt="image-20210414100722012" style="zoom:67%;" />

- 数据监控: 最基础的报表工具, 供用户自助取数,多维分析,DIY个性化数据门户
- 专题分析: 对于专题运营小二, 对类目有强烈的分析诉求, 按照分析思路组织数据, 提高数据化运营的效率和质量
- 应用分析: 对接前台系统, 实现数据的自动化; 
- 数据决策: 为高层提供宏观决策分析支撑平台, 分析历史数据规律, 预测未来发展趋势, 洞察全行业动态;